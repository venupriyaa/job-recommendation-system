# -*- coding: utf-8 -*-
"""Job_recommendation_system (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IIk_hZyIRfmJDzcfnD601DgU33Zen0SU
"""

!pip install gensim

!pip install docx2txt
!pip install pdfplumber
!pip install PyMuPDF
!pip install datasets

import pandas as pd
import fitz
import ast
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import docx2txt
import pdfplumber

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')

"""# dataset : https://huggingface.co/datasets/batuhanmtl/job-skill-set

# Dataset preprocessing
"""

from datasets import load_dataset

ds = load_dataset("batuhanmtl/job-skill-set")

from datasets import load_dataset
import pandas as pd

ds = load_dataset("batuhanmtl/job-skill-set")
df = pd.DataFrame(ds["train"])
print(df.head())
print("\nMissing values:\n", df.isnull().sum())

ds = load_dataset("batuhanmtl/job-skill-set")

# Convert to Pandas DataFrame
df = pd.DataFrame(ds["train"])

df.columns = df.columns.str.lower().str.replace(" ", "_")

df["job_skill_set"] = df["job_skill_set"].fillna("no skills mentioned")
df.dropna(subset=["job_title", "job_description"], inplace=True)

def convert_to_list(skill_string):
    if isinstance(skill_string, str):
        try:
            return ast.literal_eval(skill_string)
        except (SyntaxError, ValueError):
            return ["no skills mentioned"]
    return skill_string

df["job_skill_set"] = df["job_skill_set"].apply(convert_to_list)
df["category"] = df["category"].str.lower()

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()  # Lowercasing
    text = re.sub(r"[^\w\s]", "", text)  # Remove special characters
    text = re.sub(r"\d+", "", text)  # Remove numbers
    text = " ".join(text.split())  # Remove extra spaces
    text = " ".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])  # Lemmatization & Stopword removal
    return text

df["job_description"] = df["job_description"].apply(clean_text)

df["job_skill_set"] = df["job_skill_set"].apply(lambda x: ", ".join(x) if isinstance(x, list) else x)

df.drop_duplicates(inplace=True)

df.drop_duplicates(inplace=True)

df.to_csv("cleaned_job_dataset.csv", index=False)
print("Preprocessing complete! Saved as 'cleaned_job_dataset.csv'.")
print(df.head())

"""# Resume preprocessing"""

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file."""
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text("text") + " "  # Extract text from each page
    return text.strip()

def fix_spacing(text):
    """Fix missing spaces between words using regex."""
    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)  # Add space between camel case words
    text = re.sub(r'(?<=[a-zA-Z])(?=\d)', ' ', text)  # Add space between words and numbers
    text = re.sub(r'(?<=\d)(?=[a-zA-Z])', ' ', text)  # Add space between numbers and words
    text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)  # Add space between lowercase & uppercase transitions
    return text

def preprocess_resume(text):
    """Preprocess resume text: Fix spacing, remove unwanted characters, lemmatize, and remove stopwords."""
    text = fix_spacing(text)  # Fix missing spaces
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^a-z0-9\s]', '', text)  # Remove special characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    words = word_tokenize(text)  # Tokenize words
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization & stopword removal
    return " ".join(words)

pdf_path = "/content/Venu_Priya_Resume.pdf"
resume_text = extract_text_from_pdf(pdf_path)

cleaned_resume = preprocess_resume(resume_text)

print(" Cleaned Resume Text:")
print(cleaned_resume)

"""# why concatenation is needed"""

import pandas as pd
import numpy as np
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder


df = pd.read_csv("/content/cleaned_job_dataset.csv")
label_encoder = LabelEncoder()
df["category_encoded"] = label_encoder.fit_transform(df["category"])
text_features = ["job_title", "job_description", "job_skill_set"]


mi_scores = {}
vectorizers = {}

for feature in text_features:
    # Convert text to numerical form using TF-IDF
    vectorizer = TfidfVectorizer(max_features=5000)  # Limit features for efficiency
    X_tfidf = vectorizer.fit_transform(df[feature].astype(str))  # Convert to string
    mi_score = mutual_info_classif(X_tfidf.toarray(), df["category_encoded"], discrete_features=False)

    mi_scores[feature] = np.mean(mi_score)
    vectorizers[feature] = vectorizer


print("Mutual Information Scores:")
for feature, score in mi_scores.items():
    print(f"   - {feature}: {score:.5f}")


best_feature = max(mi_scores, key=mi_scores.get)
print(f"\n Best feature for embeddings: {best_feature}")


best_vectorizer = vectorizers[best_feature]
X_selected_feature = best_vectorizer.fit_transform(df[best_feature].astype(str))


df_tfidf = pd.DataFrame(X_selected_feature.toarray(), columns=best_vectorizer.get_feature_names_out())


df_tfidf.to_csv("selected_feature_embeddings.csv", index=False)

print("\n Feature selection complete! Using this column for further processing.")

"""#concatenation code"""

import pandas as pd

def preprocess_text(text):
    """Basic text preprocessing: lowercase, remove extra spaces."""
    if isinstance(text, str):
        return ' '.join(text.lower().strip().split())
    return ''
df = pd.read_csv("/content/cleaned_job_dataset.csv")

df['job_title_cleaned'] = df['job_title'].apply(preprocess_text)
df['job_description_cleaned'] = df['job_description'].apply(preprocess_text)
df['job_skill_set_cleaned'] = df['job_skill_set'].apply(preprocess_text)


df['combined_text'] = (
    df['job_title_cleaned'] + " " +
    df['job_description_cleaned'] + " " +
    df['job_skill_set_cleaned']
)
df.to_csv("/content/final_combined_jobs.csv", index=False)

print(df[['job_title', 'combined_text']].head())

data=pd.read_csv("/content/final_combined_jobs.csv")
print(data.head(4))

"""#tf idf and sbert + cosine embeddings"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

df = pd.read_csv("/content/final_combined_jobs.csv")
corpus = df['combined_text'].astype(str).tolist()

resume_text = cleaned_resume

def tfidf_embedding(corpus, resume_text):
    vectorizer = TfidfVectorizer(max_features=5000)
    job_embeddings = vectorizer.fit_transform(corpus).toarray()
    resume_embedding = vectorizer.transform([resume_text]).toarray()
    return job_embeddings, resume_embedding

def sbert_embedding(corpus, resume_text):
    model = SentenceTransformer("all-MiniLM-L6-v2")  # Lightweight SBERT model
    job_embeddings = model.encode(corpus)
    resume_embedding = model.encode([resume_text])
    return job_embeddings, resume_embedding

def compute_similarity(job_embeddings, resume_embedding):
    similarities = cosine_similarity(job_embeddings, resume_embedding)
    return similarities.flatten()

job_tfidf, resume_tfidf = tfidf_embedding(corpus, resume_text)
tfidf_scores = compute_similarity(job_tfidf, resume_tfidf)

job_sbert, resume_sbert = sbert_embedding(corpus, resume_text)
sbert_scores = compute_similarity(job_sbert, resume_sbert)

df['tfidf_score'] = tfidf_scores
df['sbert_score'] = sbert_scores

df_sorted = df.sort_values(by="sbert_score", ascending=False)

print("\n**Top 5 Job Recommendations (SBERT Similarity)**")
print(df_sorted[['job_title', 'sbert_score']].head(5))

print("\n **Top 5 Job Recommendations (TF-IDF Similarity)**")
print(df.sort_values(by="tfidf_score", ascending=False)[['job_title', 'tfidf_score']].head(5))

"""# Embeddings: Sentence-BERT  
# Neural Network: Multi-Layer Perceptron
"""

!pip install sentence-transformers
!pip install tensorflow
!pip install scikit-learn

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Concatenate, Input
from tensorflow.keras.optimizers import Adam
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

df = pd.read_csv("/content/final_combined_jobs.csv")

df['original_job_id'] = df['job_id']
df.reset_index(drop=True, inplace=True)

print("Generating embeddings for job listings...")
model_st = SentenceTransformer('all-MiniLM-L6-v2')
job_embeddings = model_st.encode(df['combined_text'].tolist(), show_progress_bar=True)

label_encoder = LabelEncoder()
df['category_encoded'] = label_encoder.fit_transform(df['category'])
num_categories = len(label_encoder.classes_)

print("Processing resume...")
resume_text = cleaned_resume
resume_embedding = model_st.encode([resume_text])[0]

def create_nn_matching_model(embedding_dim=384):
    """
    Create a neural network that learns to match resumes to jobs
    """
    resume_input = Input(shape=(embedding_dim,), name="resume_embedding")
    job_input = Input(shape=(embedding_dim,), name="job_embedding")

    resume_dense = Dense(256, activation='relu')(resume_input)
    resume_dropout = Dropout(0.3)(resume_dense)
    resume_dense2 = Dense(128, activation='relu')(resume_dropout)

    job_dense = Dense(256, activation='relu')(job_input)
    job_dropout = Dropout(0.3)(job_dense)
    job_dense2 = Dense(128, activation='relu')(job_dropout)


    combined = Concatenate()([resume_dense2, job_dense2])

    dense_combined = Dense(128, activation='relu')(combined)
    dropout_combined = Dropout(0.3)(dense_combined)
    dense_combined2 = Dense(64, activation='relu')(dropout_combined)


    output = Dense(1, activation='sigmoid', name="similarity_score")(dense_combined2)


    model = Model(inputs=[resume_input, job_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.001),
                 loss='binary_crossentropy',
                 metrics=['accuracy'])

    return model

def create_category_prediction_model(embedding_dim=384, num_categories=num_categories):


    resume_input = Input(shape=(embedding_dim,), name="resume_embedding")

    dense1 = Dense(256, activation='relu')(resume_input)
    dropout1 = Dropout(0.3)(dense1)
    dense2 = Dense(128, activation='relu')(dropout1)
    dropout2 = Dropout(0.3)(dense2)

    output = Dense(num_categories, activation='softmax', name="category_prediction")(dropout2)


    model = Model(inputs=resume_input, outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.001),
                 loss='sparse_categorical_crossentropy',
                 metrics=['accuracy'])

    return model

def prepare_training_data(job_embeddings, df, sample_size=10000):

    X_resume = []
    X_job = []
    y = []


    category_groups = df.groupby('category_encoded')
    categories = list(category_groups.groups.keys())


    pairs_created = 0
    while pairs_created < sample_size:

        is_positive = np.random.choice([True, False])

        if is_positive:

            category = np.random.choice(categories)

            jobs_in_category = category_groups.get_group(category)
            if len(jobs_in_category) < 2:
                continue


            job_indices = np.random.choice(jobs_in_category.index.values, 2, replace=False)


            X_resume.append(job_embeddings[job_indices[0]])
            X_job.append(job_embeddings[job_indices[1]])
            y.append(1)

        else:

            categories_sample = np.random.choice(categories, 2, replace=False)

            job1 = np.random.choice(category_groups.get_group(categories_sample[0]).index.values)
            job2 = np.random.choice(category_groups.get_group(categories_sample[1]).index.values)


            X_resume.append(job_embeddings[job1])
            X_job.append(job_embeddings[job2])
            y.append(0)

        pairs_created += 1

    return np.array(X_resume), np.array(X_job), np.array(y)

def prepare_category_data(job_embeddings, df):

    X = job_embeddings
    y = df['category_encoded'].values

    return X, y

print("Preparing training data...")
X_resume, X_job, y_matching = prepare_training_data(job_embeddings, df)

X_cat, y_cat = prepare_category_data(job_embeddings, df)

X_resume_train, X_resume_val, X_job_train, X_job_val, y_matching_train, y_matching_val = train_test_split(
    X_resume, X_job, y_matching, test_size=0.2, random_state=42)

X_cat_train, X_cat_val, y_cat_train, y_cat_val = train_test_split(
    X_cat, y_cat, test_size=0.2, stratify=y_cat, random_state=42)

print("Training job matching model...")
matching_model = create_nn_matching_model()
matching_model.fit(
    [X_resume_train, X_job_train], y_matching_train,
    validation_data=([X_resume_val, X_job_val], y_matching_val),
    epochs=10, batch_size=32, verbose=1
)

print("Training category prediction model...")
category_model = create_category_prediction_model()
category_model.fit(
    X_cat_train, y_cat_train,
    validation_data=(X_cat_val, y_cat_val),
    epochs=10, batch_size=32, verbose=1
)

def recommend_jobs_dl(resume_embedding, job_embeddings_df, matching_model, df, top_n=10):

    job_indices = job_embeddings_df.index.values
    job_embs = job_embeddings

    resume_embeddings = np.array([resume_embedding] * len(job_embs))


    similarity_scores = matching_model.predict([resume_embeddings, job_embs], verbose=0).flatten()


    job_scores = list(zip(job_indices, similarity_scores))


    job_scores.sort(key=lambda x: x[1], reverse=True)
    top_recommendations = []
    for idx, score in job_scores[:top_n]:
        job_info = df.loc[idx]
        top_recommendations.append({
            'job_id': job_info['original_job_id'],
            'job_title': job_info['job_title'],
            'category': job_info['category'],
            'skills': job_info['job_skill_set'],
            'similarity_score': float(score)
        })

    return top_recommendations

category_probs = category_model.predict(np.array([resume_embedding]), verbose=0)[0]
predicted_category_idx = np.argmax(category_probs)
predicted_category = label_encoder.inverse_transform([predicted_category_idx])[0]
category_confidence = category_probs[predicted_category_idx]

print(f"\nPredicted Resume Category: {predicted_category} (Confidence: {category_confidence:.2f})")

print("\nFinding job matches using neural network...")
dl_recommendations = recommend_jobs_dl(resume_embedding, pd.DataFrame(job_embeddings), matching_model, df)
print("\n===== TOP JOB RECOMMENDATIONS (DEEP LEARNING) =====")
for i, rec in enumerate(dl_recommendations, 1):
    print(f"\n{i}. {rec['job_title']}")
    print(f"   Category: {rec['category']}")
    print(f"   Required Skills: {rec['skills']}")
    print(f"   Match Score: {rec['similarity_score']:.4f}")

def evaluate_dl_models(matching_model, category_model, test_df, label_encoder):

    X_test = model_st.encode(test_df['combined_text'].tolist())
    y_test = test_df['category_encoded'].values

    y_pred_probs = category_model.predict(X_test, verbose=0)
    y_pred = np.argmax(y_pred_probs, axis=1)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'y_pred': y_pred,
        'y_test': y_test
    }

from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)


evaluation_results = evaluate_dl_models(matching_model, category_model, test_df, label_encoder)

evaluation_results = evaluate_dl_models(matching_model, category_model, test_df, label_encoder)

print("\n===== MODEL EVALUATION =====")
print(f"- Accuracy: {evaluation_results['accuracy']:.4f}")
print(f"- Precision: {evaluation_results['precision']:.4f}")
print(f"- Recall: {evaluation_results['recall']:.4f}")
print(f"- F1 Score: {evaluation_results['f1']:.4f}")


y_pred_labels = label_encoder.inverse_transform(evaluation_results['y_pred'])
y_true_labels = label_encoder.inverse_transform(evaluation_results['y_test'])

matching_model.export('/content/job_matching_model')
category_model.export('/content/job_category_model')

print("\nModels saved to '/content/job_matching_model' and '/content/job_category_model'")

"""#SBERT FINE TUNING"""

!pip install -U sentence-transformers datasets scikit-learn

from datasets import load_dataset
import pandas as pd

# Load dataset
dataset = load_dataset("batuhanmtl/job-skill-set")

# We'll use only the 'train' split
df = pd.DataFrame(dataset['train'])
df = df[['job_title', 'job_description']].dropna().reset_index(drop=True)

print(df.head())

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]
    return " ".join(tokens)

df['job_description'] = df['job_description'].apply(clean_text)

from sentence_transformers import InputExample
import random

triplets = []
grouped = df.groupby("job_title")

for idx, row in df.iterrows():
    anchor = row['job_description']
    job_title = row['job_title']

    pos_pool = grouped.get_group(job_title).drop(index=idx, errors='ignore')
    if pos_pool.empty:
        continue
    positive = pos_pool.sample(1).iloc[0]['job_description']

    neg_pool = df[df['job_title'] != job_title]
    negative = neg_pool.sample(1).iloc[0]['job_description']

    triplets.append(InputExample(texts=[anchor, positive, negative]))

print(f"Generated {len(triplets)} triplets.")

from sentence_transformers import InputExample
import random

triplets = []
grouped = df.groupby("job_title")

for idx, row in df.iterrows():
    anchor = row['job_description']
    job_title = row['job_title']

    pos_pool = grouped.get_group(job_title).drop(index=idx, errors='ignore')
    if pos_pool.empty:
        continue
    positive = pos_pool.sample(1).iloc[0]['job_description']

    neg_pool = df[df['job_title'] != job_title]
    negative = neg_pool.sample(1).iloc[0]['job_description']

    triplets.append(InputExample(texts=[anchor, positive, negative]))

print(f"Generated {len(triplets)} triplets.")

from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, losses

model = SentenceTransformer('all-MiniLM-L6-v2')
train_dataloader = DataLoader(triplets, shuffle=True, batch_size=8)
train_loss = losses.TripletLoss(model=model)

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=2,
    warmup_steps=100,
    show_progress_bar=True
)

model.save("fine-tuned-sbert-job-skill")
print("Model saved!")

from sentence_transformers import SentenceTransformer

# Load your fine-tuned model
model = SentenceTransformer("fine-tuned-sbert-job-skill")

# Generate embeddings for all job entries
df=pd.read_csv("/content/final_combined_jobs.csv")
job_embeddings = model.encode(df['combined_text'].tolist(), show_progress_bar=True, convert_to_tensor=True)

resume_embedding = model.encode([cleaned_resume], convert_to_tensor=True)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate
from tensorflow.keras.optimizers import Adam
import numpy as np
import pandas as pd

# Encode categories
label_encoder = LabelEncoder()
df['category_encoded'] = label_encoder.fit_transform(df['category'])
num_categories = len(label_encoder.classes_)

def create_nn_matching_model(embedding_dim=384):
    resume_input = Input(shape=(embedding_dim,), name="resume_embedding")
    job_input = Input(shape=(embedding_dim,), name="job_embedding")

    resume_dense = Dense(256, activation='relu')(resume_input)
    resume_dropout = Dropout(0.3)(resume_dense)
    resume_dense2 = Dense(128, activation='relu')(resume_dropout)

    job_dense = Dense(256, activation='relu')(job_input)
    job_dropout = Dropout(0.3)(job_dense)
    job_dense2 = Dense(128, activation='relu')(job_dropout)

    combined = Concatenate()([resume_dense2, job_dense2])
    dense_combined = Dense(128, activation='relu')(combined)
    dropout_combined = Dropout(0.3)(dense_combined)
    dense_combined2 = Dense(64, activation='relu')(dropout_combined)

    output = Dense(1, activation='sigmoid', name="similarity_score")(dense_combined2)

    model = Model(inputs=[resume_input, job_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

def create_category_prediction_model(embedding_dim=384, num_categories=num_categories):
    resume_input = Input(shape=(embedding_dim,), name="resume_embedding")
    dense1 = Dense(256, activation='relu')(resume_input)
    dropout1 = Dropout(0.3)(dense1)
    dense2 = Dense(128, activation='relu')(dropout1)
    dropout2 = Dropout(0.3)(dense2)

    output = Dense(num_categories, activation='softmax', name="category_prediction")(dropout2)

    model = Model(inputs=resume_input, outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

def prepare_training_data(job_embeddings, df, sample_size=10000):
    X_resume, X_job, y = [], [], []
    category_groups = df.groupby('category_encoded')
    categories = list(category_groups.groups.keys())

    pairs_created = 0
    while pairs_created < sample_size:
        is_positive = np.random.choice([True, False])
        if is_positive:
            category = np.random.choice(categories)
            jobs_in_category = category_groups.get_group(category)
            if len(jobs_in_category) < 2:
                continue
            job_indices = np.random.choice(jobs_in_category.index.values, 2, replace=False)
            X_resume.append(job_embeddings[job_indices[0]])
            X_job.append(job_embeddings[job_indices[1]])
            y.append(1)
        else:
            categories_sample = np.random.choice(categories, 2, replace=False)
            job1 = np.random.choice(category_groups.get_group(categories_sample[0]).index.values)
            job2 = np.random.choice(category_groups.get_group(categories_sample[1]).index.values)
            X_resume.append(job_embeddings[job1])
            X_job.append(job_embeddings[job2])
            y.append(0)
        pairs_created += 1
    return np.array(X_resume), np.array(X_job), np.array(y)

def prepare_category_data(job_embeddings, df):
    X = job_embeddings
    y = df['category_encoded'].values
    return X, y

# Prepare data
print("Preparing training data...")
X_resume, X_job, y_matching = prepare_training_data(job_embeddings, df)
X_cat, y_cat = prepare_category_data(job_embeddings, df)

# Split
X_resume_train, X_resume_val, X_job_train, X_job_val, y_matching_train, y_matching_val = train_test_split(
    X_resume, X_job, y_matching, test_size=0.2, random_state=42)
X_cat_train, X_cat_val, y_cat_train, y_cat_val = train_test_split(
    X_cat, y_cat, test_size=0.2, stratify=y_cat, random_state=42)

# Train matching model
print("Training job matching model...")
matching_model = create_nn_matching_model()
matching_model.fit(
    [X_resume_train, X_job_train], y_matching_train,
    validation_data=([X_resume_val, X_job_val], y_matching_val),
    epochs=10, batch_size=32, verbose=1
)

# Train category model
print("Training category prediction model...")
category_model = create_category_prediction_model()
category_model.fit(
    X_cat_train, y_cat_train,
    validation_data=(X_cat_val, y_cat_val),
    epochs=10, batch_size=32, verbose=1
)

# Recommendation function
def recommend_jobs_dl(resume_embedding, job_embeddings_df, matching_model, df, top_n=10):
    job_indices = job_embeddings_df.index.values
    job_embs = job_embeddings_df.values  # (N, 384)

    resume_embedding = resume_embedding.reshape(1, -1)  # Ensure shape (1, 384)
    resume_embeddings = np.repeat(resume_embedding, len(job_embs), axis=0)  # (N, 384)

    similarity_scores = matching_model.predict([resume_embeddings, job_embs], verbose=0).flatten()
    job_scores = list(zip(job_indices, similarity_scores))
    job_scores.sort(key=lambda x: x[1], reverse=True)

    top_recommendations = []
    for idx, score in job_scores[:top_n]:
        job_info = df.loc[idx]
        top_recommendations.append({
            'job_id': job_info['job_id'],
            'job_title': job_info['job_title'],
            'category': job_info['category'],
            'skills': job_info['job_skill_set'],
            'similarity_score': float(score)
        })
    return top_recommendations

# Run a sample prediction
resume_embedding = resume_embedding.reshape(1, -1)  # Ensure it's the correct shape
category_probs = category_model.predict(resume_embedding, verbose=0)[0]
predicted_category_idx = np.argmax(category_probs)
predicted_category = label_encoder.inverse_transform([predicted_category_idx])[0]
category_confidence = category_probs[predicted_category_idx]

print(f"\nPredicted Category: {predicted_category} ({category_confidence:.2f} confidence)")
print("\nFinding job matches using neural network...")

job_embeddings_df = pd.DataFrame(job_embeddings)  # Wrap job_embeddings in DataFrame
dl_recommendations = recommend_jobs_dl(resume_embedding, job_embeddings_df, matching_model, df)

print("\n===== TOP JOB RECOMMENDATIONS (DEEP LEARNING) =====")
for i, rec in enumerate(dl_recommendations, 1):
    print(f"\n{i}. {rec['job_title']}")
    print(f"   Category: {rec['category']}")
    print(f"   Required Skills: {rec['skills']}")
    print(f"   Match Score: {rec['similarity_score']:.4f}")

# Prepare test_df for evaluation
test_df = df.sample(frac=0.2, random_state=42).copy()
test_df['combined_text'] = test_df['job_title'] + " " + test_df['job_description'] + " " + test_df['job_skill_set']
test_df['category_encoded'] = label_encoder.transform(test_df['category'])

# Evaluation
def evaluate_dl_models(category_model, test_df, label_encoder):
    X_test = model.encode(test_df['combined_text'].tolist())
    y_test = test_df['category_encoded'].values

    y_pred_probs = category_model.predict(X_test, verbose=0)
    y_pred = np.argmax(y_pred_probs, axis=1)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'y_pred': y_pred,
        'y_test': y_test
    }

evaluation_results = evaluate_dl_models(category_model, test_df, label_encoder)

print("\n===== MODEL EVALUATION =====")
print(f"- Accuracy: {evaluation_results['accuracy']:.4f}")
print(f"- Precision: {evaluation_results['precision']:.4f}")
print(f"- Recall: {evaluation_results['recall']:.4f}")
print(f"- F1 Score: {evaluation_results['f1']:.4f}")